version: '2'
services:
  fake:
    build:
      context: ./
      dockerfile: data_sources/fake/Dockerfile
  dblp:
    build:
      context: ./
      dockerfile: data_sources/dblp/Dockerfile
    volumes:
      - ./data/:/data/:ro
  zookeeper:
    image: bitnami/zookeeper:3
    ports:
      - 2181:2181
    volumes:
      - zookeeper_data:/bitnami
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
  kafka:
    image: bitnami/kafka:2
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
  kafka1:
    extends:
      service: kafka
    volumes:
      - kafka1_data:/bitnami
    depends_on:
      - zookeeper
  kafka2:
    extends:
      service: kafka
    volumes:
      - kafka2_data:/bitnami
    depends_on:
      - zookeeper
  dashboard:
    build:
      context: ./
      dockerfile: ./dashboard/Dockerfile
    ports:
      - 8050:8050
  spark:
    image: vmalashkov/apache-spark:spark-3.1.2-h3.2-py-3.8.12
  driver:
    extends:
      service: spark
    environment:
      - MODE=LOCAL
      - SPARK_CONF_DIR=/spark-conf
    volumes:
      - ./conf/:/spark-conf/
  etl:
    build:
      context: ./
      dockerfile: ./etl/Dockerfile
    environment:
      - MODE=STANDALONE
      - SPARK_MASTER_HOST=spark-master
      - SPARK_CONF_DIR=/spark-conf
    ports:
      - 4041:4040
    volumes:
      - ./conf/:/spark-conf/
  clustering:
    build:
      context: ./
      dockerfile: analytics/Dockerfile
    environment:
#      - MODE=STANDALONE
#      - SPARK_MASTER_HOST=spark-master
      - SPARK_CONF_DIR=/spark-conf
    volumes:
      - ./conf/:/spark-conf/
  reporting:
    build:
      context: ./
      dockerfile: analytics/Dockerfile
    command: "python /usr/src/app/reporting.py"
    environment:
      - MODE=STANDALONE
      - SPARK_MASTER_HOST=spark-master
  notebook:
    build:
      context: ./analysis
      dockerfile: Dockerfile
    command: spark-jupyter
    environment:
      - MODE=STANDALONE
      - SPARK_MASTER_HOST=spark-master
    ports:
      - 7000:7000
      - 4040:4040
    volumes:
      - ./analysis:/notebooks
  spark-master:
    extends:
      service: spark
    depends_on:
      - spark-worker1
      - spark-worker2
    command: master
    ports:
      - 8080:8080
      - 7077:7077
  spark-worker1:
    extends:
      service: spark
    command: worker
    environment:
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_WORKER_PORT=7078
      - SPARK_MASTER_HOST=spark-master
    ports:
      - 8081:8081
      - 7078:7078
  spark-worker2:
    extends:
      service: spark
    command: worker
    environment:
      - SPARK_WORKER_WEBUI_PORT=8082
      - SPARK_WORKER_PORT=7079
      - SPARK_MASTER_HOST=spark-master
    ports:
      - 8082:8082
      - 7079:7079
  reporting-db:
    image: postgres:12
    environment:
      - POSTGRES_USER=login
      - POSTGRES_PASSWORD=pwd
      - POSTGRES_DB=reporting
    ports:
      - 5432:5432
    volumes:
      - reporting_data:/var/lib/postgresql/data
  minio:
    image: minio/minio
    ports:
      - 9000:9000
      - 9001:9001
    volumes:
      - minio_data:/data
    environment:
      MINIO_ACCESS_KEY: minio_access_key
      MINIO_SECRET_KEY: minio_secret_key
    command: server /data --console-address ":9001"
  minio-mc:
    image: minio/mc
    environment:
      MINIO_ACCESS_KEY: minio_access_key
      MINIO_SECRET_KEY: minio_secret_key
    entrypoint: ['']
    command: ['echo', 'nothing']
volumes:
  reporting_data:
    driver: local
  zookeeper_data:
    driver: local
  kafka1_data:
    driver: local
  kafka2_data:
    driver: local
  minio_data:
    driver: local
